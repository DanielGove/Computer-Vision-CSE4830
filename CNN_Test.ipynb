{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-JKxvDFQ_mEY"
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4g6EiVsAXIQ"
   },
   "source": [
    "# Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c4LoQYwW_5dT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class VHRDataset(Dataset):\n",
    "    def __init__(self, positive_img_dir, negative_img_dir, gt_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            positive_img_dir (string): Directory with all the positive images.\n",
    "            negative_img_dir (string): Directory with all the negative images.\n",
    "            gt_dir (string): Directory with all the ground truth files for positive images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.positive_img_dir = positive_img_dir\n",
    "        self.negative_img_dir = negative_img_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # List of all images and their types (positive/negative)\n",
    "        self.imgs = [(os.path.join(positive_img_dir, f), 'positive') for f in os.listdir(positive_img_dir) if f.endswith('.jpg')]\n",
    "        self.imgs += [(os.path.join(negative_img_dir, f), 'negative') for f in os.listdir(negative_img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, img_type = self.imgs[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if img_type == 'positive':\n",
    "            gt_path = os.path.join(self.gt_dir, os.path.basename(img_path).replace('.jpg', '.txt'))\n",
    "            with open(gt_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    if line == '\\n':\n",
    "                        continue\n",
    "                    x1, y1, x2, y2, a = line.strip().split(',')\n",
    "                    x1, x2 = x1.strip('('), x2.strip('(')\n",
    "                    y1, y2 = y1.strip(')').strip(), y2.strip(')').strip()\n",
    "                    boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "                    labels.append(int(a))\n",
    "\n",
    "        sample = {'image': image, 'boxes': boxes, 'labels': labels}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class MyTransform:\n",
    "    \"\"\"Apply transformations to the image and adjust bounding boxes.\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['boxes'], sample['labels']\n",
    "        w, h = image.size\n",
    "        new_h, new_w = self.output_size if isinstance(self.output_size, tuple) else (self.output_size, self.output_size)\n",
    "\n",
    "        # Resize the image\n",
    "        image = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "        # Scale the bounding boxes\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            scaled_box = [\n",
    "                box[0] * new_w / w, box[1] * new_h / h,\n",
    "                box[2] * new_w / w, box[3] * new_h / h\n",
    "            ]\n",
    "            new_boxes.append(scaled_box)\n",
    "\n",
    "        # Convert image eto tensor and normalize\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        return {'image': image, 'boxes': torch.tensor(new_boxes, dtype=torch.float32), 'labels': torch.tensor(labels, dtype=torch.int8)}\n",
    "\n",
    "# Used to combine several samples into a single batched tensor to be processed by the model.\n",
    "def collate_fn(batch):\n",
    "    images = [item['image'] for item in batch]\n",
    "    boxes = [item['boxes'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad boxes and labels to the maximum length so that they are of uniform size\n",
    "    max_boxes_len = max(len(b) for b in boxes) # 25\n",
    "    max_boxes = 67\n",
    "    assert max_boxes_len <= max_boxes, f\"Oh shit, {max_boxes_len} labels\"\n",
    "    padded_boxes = []\n",
    "    padded_labels = []\n",
    "    for b, l in zip(boxes, labels):\n",
    "        if b.ndim == 2:\n",
    "            b = torch.nn.functional.pad(b, (0, 0, 0, max_boxes - b.size(0)))\n",
    "            l = torch.nn.functional.pad(l, (0, max_boxes - l.size(0)))\n",
    "        else:\n",
    "            b = torch.zeros((max_boxes, 4))\n",
    "            l = torch.zeros((max_boxes,  ))\n",
    "    \n",
    "        padded_boxes.append(b)\n",
    "        padded_labels.append(l)\n",
    "    \n",
    "    # Convert padded lists to tensors\n",
    "    padded_boxes = torch.stack(padded_boxes)\n",
    "    padded_labels = torch.stack(padded_labels)\n",
    "\n",
    "    return {'image': torch.stack(images), 'boxes': padded_boxes, 'labels': padded_labels}\n",
    "\n",
    "dataset = VHRDataset(\n",
    "    positive_img_dir='/home/dan/projects/cse4830/RI-CNN/NWPU VHR-10 dataset/positive image set',\n",
    "    negative_img_dir='/home/dan/projects/cse4830/RI-CNN/NWPU VHR-10 dataset/negative image set',\n",
    "    gt_dir='/home/dan/projects/cse4830/RI-CNN/NWPU VHR-10 dataset/ground truth',\n",
    "    transform = MyTransform((512, 384))\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "This model is a \"Single Shot Detector\" which means it tries to predict both the bounding\n",
    "boxes and the class labels at the time time. Earlier architectures would first try to\n",
    "predict the regions that contained objects, and then those regions would be classified.\n",
    "This was very complicated and not as fast as a single shot approach.\n",
    "\"\"\"\n",
    "\n",
    "class ObjectDetectionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ObjectDetectionCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers for bounding box regression\n",
    "        self.fc1_bbox = nn.Linear(128 * 48 * 64, 500)\n",
    "        self.fc2_bbox = nn.Linear(500, 4 * 67)\n",
    "\n",
    "        # Fully connected layers for class prediction\n",
    "        self.fc1_cls = nn.Linear(128 * 48 * 64, 500)\n",
    "        self.fc2_cls = nn.Linear(500, num_classes * 67)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        x = self.pool(F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x))\n",
    "        x = self.pool(F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten the feature map\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Bounding box regression\n",
    "        bbox = F.relu(self.fc1_bbox(x))\n",
    "        bbox = self.FC2_bbox(bbox)\n",
    "        bbox = bbox.view(-1, 67, 4)\n",
    "\n",
    "        # Class prediction\n",
    "        cls = F.relu(self.fc1_cls(x))\n",
    "        cls = self.fc2_cls(x)\n",
    "        cls = cls.view(-1, 67, num_classes)\n",
    "\n",
    "        return bbox, cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h9MSuOApyuer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n",
      "torch.Size([4, 3, 512, 384]) torch.Size([4, 67, 4]) torch.Size([4, 67])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    images = batch['image']\n",
    "    boxes = batch['boxes']\n",
    "    labels = batch['labels']\n",
    "    # Process images, boxes, and labels\n",
    "    # Example: forward pass in your model\n",
    "\n",
    "    print(images.shape, boxes.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "anjQhPt4CyGl",
    "outputId": "0de27525-4633-4c54-b882-6fb57632b639"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Dataset/Annotations/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-228a912fcc0c>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0406e586fdc3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, split, transform)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Annotations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Dataset/Annotations/train'"
     ]
    }
   ],
   "source": [
    "def visualize_image_with_annotations(image, annotations):\n",
    "    to_pil = ToPILImage()\n",
    "    # Convert tensor image to PIL image\n",
    "    image_pil = to_pil(image)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image_pil)\n",
    "\n",
    "    # Plot annotations\n",
    "    for annotation in annotations:\n",
    "        category_id = annotation['category_id']\n",
    "        bbox = annotation['poly']\n",
    "\n",
    "        # Extracting x and y coordinates separately\n",
    "        x_coords = [bbox[i] for i in range(0, len(bbox), 2)]\n",
    "        y_coords = [bbox[i] for i in range(1, len(bbox), 2)]\n",
    "\n",
    "        # Convert coordinates to list explicitly\n",
    "        x_coords = list(x_coords)\n",
    "        y_coords = list(y_coords)\n",
    "\n",
    "        # Create a Polygon patch\n",
    "        polygon = patches.Polygon(list(zip(x_coords, y_coords)), linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(polygon)\n",
    "\n",
    "        # Add category label\n",
    "        ax.text(min(x_coords), min(y_coords), str(category_id), fontsize=10, color='w', verticalalignment='bottom')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "root_dir = '/content/drive/My Drive/Dataset'\n",
    "train_dataset = CustomDataset(root_dir, split='train', transform=transform)\n",
    "\n",
    "x = train_dataset.__getitem__(1)\n",
    "image_tensor = x[0]\n",
    "#print(image_tensor)\n",
    "annotations = x[1]\n",
    "#print(annotations)\n",
    "\n",
    "# Visualize the image with bounding box annotations\n",
    "visualize_image_with_annotations(image_tensor, annotations['annotations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXCL-j1TDgn0"
   },
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y36NyfLvDjLF"
   },
   "outputs": [],
   "source": [
    "class CustomDatasetV2(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load annotation file names for the specified split\n",
    "        annotation_dir = os.path.join(root_dir, 'Annotations', split)\n",
    "        self.annotation_files = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_name = self.annotation_files[idx]\n",
    "        img_name = annotation_name.replace('.json', '.jpg')\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.root_dir, 'Images', img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load JSON annotations\n",
    "        annotation_path = os.path.join(self.root_dir, 'Annotations', self.split, annotation_name)\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "        # Extract bounding box coordinates and class labels\n",
    "        bbox_coords = []\n",
    "        class_labels = []\n",
    "        for annotation in annotations['annotations']:\n",
    "            bbox = annotation['poly']\n",
    "            bbox_coords.append(bbox)\n",
    "            class_labels.append(annotation['category_id'])\n",
    "\n",
    "        target = {\n",
    "            'boxes': bbox_coords,\n",
    "            'labels': class_labels\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            target = self.adjust_bbox(target, image.size)\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "    def adjust_bbox(self, target, new_size):\n",
    "        size = 224\n",
    "        new_width, new_height = new_size\n",
    "\n",
    "        # Calculate scaling factors\n",
    "        width_ratio = size / new_width  # Adjust accordingly if resizing\n",
    "        height_ratio = size / new_height  # Adjust accordingly if resizing\n",
    "\n",
    "        adjusted_bboxes = []\n",
    "        for bbox in target['boxes']:\n",
    "            adjusted_bbox = []\n",
    "            for i in range(0, len(bbox), 2):  # Iterate over x and y coordinates separately\n",
    "                x_coord = bbox[i] * width_ratio\n",
    "                y_coord = bbox[i + 1] * height_ratio\n",
    "                adjusted_bbox.extend([x_coord, y_coord])  # Add adjusted coordinates\n",
    "            adjusted_bboxes.append(adjusted_bbox)\n",
    "\n",
    "        return {'boxes': adjusted_bboxes, 'labels': target['labels']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dy34l4WLFL9t",
    "outputId": "24f4132a-6383-4720-a100-a778feaa87ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "tensor([[[-1.1932, -1.1418, -1.1589,  ..., -0.7993, -0.6281, -0.6281],\n",
      "         [-1.1760, -1.1418, -1.1589,  ..., -0.8849, -0.7137, -0.6452],\n",
      "         [-1.1589, -1.1075, -1.1418,  ..., -0.9534, -0.7308, -0.3541],\n",
      "         ...,\n",
      "         [ 0.0398,  0.1083,  0.1254,  ..., -1.0904, -1.1247, -1.1760],\n",
      "         [ 0.0912,  0.1083,  0.1083,  ..., -1.0904, -1.1247, -1.1247],\n",
      "         [ 0.0741,  0.0912,  0.0912,  ..., -1.0904, -1.1075, -1.1075]],\n",
      "\n",
      "        [[-0.7577, -0.7052, -0.7227,  ..., -0.5476, -0.3375, -0.4076],\n",
      "         [-0.7402, -0.7052, -0.7052,  ..., -0.5826, -0.4426, -0.4076],\n",
      "         [-0.7052, -0.6877, -0.7052,  ..., -0.6702, -0.6001, -0.3200],\n",
      "         ...,\n",
      "         [-0.0224,  0.0476,  0.0476,  ..., -0.7927, -0.8102, -0.8452],\n",
      "         [-0.0049,  0.0301,  0.0126,  ..., -0.7927, -0.8102, -0.8277],\n",
      "         [-0.0049,  0.0126, -0.0049,  ..., -0.8102, -0.8102, -0.7927]],\n",
      "\n",
      "        [[-1.0201, -0.9853, -1.0201,  ..., -0.5495, -0.2707, -0.2532],\n",
      "         [-1.0201, -1.0027, -0.9853,  ..., -0.6193, -0.4101, -0.2881],\n",
      "         [-1.0027, -0.9853, -0.9853,  ..., -0.7064, -0.6193, -0.3753],\n",
      "         ...,\n",
      "         [-0.1487, -0.0790, -0.0615,  ..., -0.8633, -0.8807, -0.9156],\n",
      "         [-0.0964, -0.0790, -0.0964,  ..., -0.8633, -0.8807, -0.8981],\n",
      "         [-0.0790, -0.0964, -0.0964,  ..., -0.8807, -0.8807, -0.8633]]])\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/content/drive/My Drive/Dataset'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust size as needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = CustomDatasetV2(root_dir, split='train', transform=transform)\n",
    "val_dataset = CustomDatasetV2(root_dir, split='val', transform=transform)\n",
    "test_dataset = CustomDatasetV2(root_dir, split='test', transform=transform)\n",
    "print(len(train_dataset.__getitem__(6)[1]['labels']))\n",
    "print((train_dataset.__getitem__(2)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaYNzI03IsdU"
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXBH-xY8-D3A"
   },
   "source": [
    "# Basic CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix_mXNZ2NyiC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BBoxClassificationCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BBoxClassificationCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "FimQb78xCSkM",
    "outputId": "e6aa69b4-f390-488c-83db-3c6f70e4f86f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-3ea0f33c11c2>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update total using the number of labels per image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Define model, loss function, and optimizer\n",
    "model = BBoxClassificationCNN(10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        num_bboxes = len(targets[1]['labels'])  # Get the number of bounding boxes\n",
    "        num_outputs = outputs.size(0)  # Get the batch size of the model's output\n",
    "\n",
    "        # Ensure that the number of bounding boxes matches the batch size of the model's output\n",
    "        if num_bboxes != num_outputs:\n",
    "            continue  # Skip this batch if the number of bounding boxes doesn't match\n",
    "\n",
    "        labels = torch.tensor(targets[1]['labels'], dtype=torch.int64)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(val_loader, desc=\"Evaluation\", leave=False):\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += len(targets[1]['labels'])  # Update total using the number of labels per image\n",
    "        correct += (predicted == torch.tensor(targets['labels'])).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on evaluation set: {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
