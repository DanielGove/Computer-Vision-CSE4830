{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-JKxvDFQ_mEY"
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4g6EiVsAXIQ"
   },
   "source": [
    "# Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c4LoQYwW_5dT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class VHRDataset(Dataset):\n",
    "    def __init__(self, positive_img_dir, negative_img_dir, gt_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            positive_img_dir (string): Directory with all the positive images.\n",
    "            negative_img_dir (string): Directory with all the negative images.\n",
    "            gt_dir (string): Directory with all the ground truth files for positive images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.positive_img_dir = positive_img_dir\n",
    "        self.negative_img_dir = negative_img_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # List of all images and their types (positive/negative)\n",
    "        self.imgs = [(os.path.join(positive_img_dir, f), 'positive') for f in os.listdir(positive_img_dir) if f.endswith('.jpg')]\n",
    "        self.imgs += [(os.path.join(negative_img_dir, f), 'negative') for f in os.listdir(negative_img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, img_type = self.imgs[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if img_type == 'positive':\n",
    "            gt_path = os.path.join(self.gt_dir, os.path.basename(img_path).replace('.jpg', '.txt'))\n",
    "            with open(gt_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    if line == '\\n':\n",
    "                        continue\n",
    "                    x1, y1, x2, y2, a = line.strip().split(',')\n",
    "                    x1, x2 = x1.strip('('), x2.strip('(')\n",
    "                    y1, y2 = y1.strip(')').strip(), y2.strip(')').strip()\n",
    "                    boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "                    labels.append(int(a))\n",
    "\n",
    "        sample = {'image': image, 'boxes': boxes, 'labels': labels}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class MyTransform:\n",
    "    \"\"\"Apply transformations to the image and adjust bounding boxes.\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes, labels = sample['image'], sample['boxes'], sample['labels']\n",
    "        w, h = image.size\n",
    "        new_h, new_w = self.output_size if isinstance(self.output_size, tuple) else (self.output_size, self.output_size)\n",
    "\n",
    "        # Resize the image\n",
    "        image = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "        # Scale the bounding boxes\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            scaled_box = [\n",
    "                box[0] * new_w / w, box[1] * new_h / h,\n",
    "                box[2] * new_w / w, box[3] * new_h / h\n",
    "            ]\n",
    "            new_boxes.append(scaled_box)\n",
    "\n",
    "        # Convert image eto tensor and normalize\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        return {'image': image, 'boxes': torch.tensor(new_boxes, dtype=torch.float32), 'labels': torch.tensor(labels, dtype=torch.int8)}\n",
    "\n",
    "# Used to combine several samples into a single batched tensor to be processed by the model.\n",
    "def collate_fn(batch):\n",
    "    images = [item['image'] for item in batch]\n",
    "    boxes = [item['boxes'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad boxes and labels to the maximum length so that they are of uniform size\n",
    "    max_boxes_len = max(len(b) for b in boxes) # 25\n",
    "    max_boxes = 67\n",
    "    assert max_boxes_len <= max_boxes, f\"Oh shit, {max_boxes_len} labels\"\n",
    "    padded_boxes = []\n",
    "    padded_labels = []\n",
    "    for b, l in zip(boxes, labels):\n",
    "        if b.ndim == 2:\n",
    "            b = torch.nn.functional.pad(b, (0, 0, 0, max_boxes - b.size(0)))\n",
    "            l = torch.nn.functional.pad(l, (0, max_boxes - l.size(0)))\n",
    "        else:\n",
    "            b = torch.zeros((max_boxes, 4))\n",
    "            l = torch.zeros((max_boxes,  ))\n",
    "    \n",
    "        padded_boxes.append(b)\n",
    "        padded_labels.append(l)\n",
    "    \n",
    "    # Convert padded lists to tensors\n",
    "    padded_boxes = torch.stack(padded_boxes)\n",
    "    padded_labels = torch.stack(padded_labels)\n",
    "\n",
    "    return {'image': torch.stack(images), 'boxes': padded_boxes, 'labels': padded_labels}\n",
    "\n",
    "dataset = VHRDataset(\n",
    "    positive_img_dir='/home/dan/projects/cse4830/RI-CNN/NWPU VHR-10 dataset/positive image set',\n",
    "    negative_img_dir='/home/dan/projects/cse4830/RI-CNN/NWPU VHR-10 dataset/negative image set',\n",
    "    gt_dir='/home/dan/projects/cse4830/RI-CNN/NWPU VHR-10 dataset/ground truth',\n",
    "    transform = MyTransform((512, 384))\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from RI_CNN import *\n",
    "\n",
    "\"\"\"\n",
    "This model is a \"Single Shot Detector\" which means it tries to predict both the bounding\n",
    "boxes and the class labels at the time time. Earlier architectures would first try to\n",
    "predict the regions that contained objects, and then those regions would be classified.\n",
    "This was very complicated and not as fast as a single shot approach.\n",
    "\"\"\"\n",
    "\n",
    "# Standard model with conventional convolutional layers\n",
    "class MrEngineerMan(nn.Module):\n",
    "    def __init__(self, img_height=384, img_width=512, num_boxes=67, num_classes=11):\n",
    "        super(MrEngineerMan, self).__init__()\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "        # Standard Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # This will reduce the dimension by half each time it's applied\n",
    "\n",
    "        # For better gradients\n",
    "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Calculate the output size after convolutions and pooling\n",
    "        out_size = self.calculate_conv_output_size()\n",
    "\n",
    "        # Dense Layers\n",
    "        self.fc1_bbox = nn.Linear(out_size, 256)\n",
    "        self.fc2_bbox = nn.Linear(256, num_boxes * 4)\n",
    "        self.fc1_class = nn.Linear(out_size, 256)\n",
    "        self.fc2_class = nn.Linear(256, num_boxes * num_classes)\n",
    "        self.fc1_conf = nn.Linear(out_size, 256)\n",
    "        self.fc2_conf = nn.Linear(256, num_boxes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.pool(x))  # Apply pooling and activation\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.pool(x))  # Apply second pooling and activation\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.pool(x))  # Third pooling and activation\n",
    "        x = self.batch_norm3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for dense layers\n",
    "\n",
    "        bbox = F.relu(self.fc1_bbox(x))\n",
    "        cls = F.relu(self.fc1_class(x))\n",
    "        conf = F.relu(self.fc1_conf(x))\n",
    "        \n",
    "        bbox = F.relu(self.fc2_bbox(bbox)).view(8, self.num_boxes, -1)\n",
    "        cls = F.relu(self.fc2_class(cls)).view(8, self.num_boxes, -1)\n",
    "        conf = F.relu(self.fc2_conf(conf)).unsqueeze(-1)\n",
    "\n",
    "        cls = F.softmax(cls)\n",
    "        conf = F.sigmoid(conf)\n",
    "        \n",
    "        return torch.cat((bbox, conf, cls), dim=-1)\n",
    "\n",
    "    def calculate_conv_output_size(self):\n",
    "        size = (self.img_height // 2 // 2 // 2, self.img_width // 2 // 2 // 2)\n",
    "        return size[0] * size[1] * 64\n",
    "    \n",
    "# Special model with Vector based convolutional layers\n",
    "class SpecialEngineerMan(nn.Module):\n",
    "    def __init__(self, img_height=384, img_width=512, num_boxes=67, num_classes=10):\n",
    "        super(SpecialEngineerMan, self).__init__()\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "        # Standard Convolutional Layers\n",
    "        self.conv1 = VectorTransformConv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = VectorConv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = VectorConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = VectorMaxPool2d(2, 2)  # This will reduce the dimension by half each time it's applied\n",
    "\n",
    "        # For better gradients\n",
    "        self.batch_norm1 = VectorBatchNorm2d(16)\n",
    "        self.batch_norm2 = VectorBatchNorm2d(32)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Calculate the output size after convolutions and pooling\n",
    "        out_size = self.calculate_conv_output_size()\n",
    "\n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(out_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_boxes * (4 + 1 + num_classes))  # 4 for bbox, 1 for confidence, num_classes for class probs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = vector_relu(self.pool(x))  # Apply pooling and activation\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = vector_relu(self.pool(x))  # Apply second pooling and activation\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = vector_relu(self.pool(x))  # Third pooling and activation\n",
    "\n",
    "        # Drop the phase and normalize\n",
    "        x = x[..., 0]\n",
    "        x = self.batch_norm3(x)\n",
    "\n",
    "        # The decision head\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x.view(-1, self.num_boxes, 5 + self.num_classes)  # Reshape for output format\n",
    "\n",
    "    def calculate_conv_output_size(self):\n",
    "        size = (self.img_height // 2 // 2 // 2, self.img_width // 2 // 2 // 2)\n",
    "        return size[0] * size[1] * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "h9MSuOApyuer"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/projects/cse4830/RI-CNN/dataloader.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_boxes[i, :num_boxes] = torch.tensor(boxes[i], dtype=torch.float32)\n",
      "/home/dan/projects/cse4830/RI-CNN/dataloader.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_labels[i, :num_boxes] = torch.tensor(labels[i], dtype=torch.int64)\n",
      "/home/dan/projects/cse4830/RI-CNN/dataloader.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_confidences[i, :num_boxes] = torch.tensor(confidences[i], dtype=torch.float32)\n",
      "/tmp/ipykernel_8604/120382616.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls = F.softmax(cls)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3000604212284088 1285.395263671875 5.8289594650268555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8604/3186478530.py:38: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 7.75 GiB of which 1.56 MiB is free. Process 6608 has 5.63 GiB memory in use. Including non-PyTorch memory, this process has 2.10 GiB memory in use. Of the allocated memory 1.91 GiB is allocated by PyTorch, and 5.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWisdom stored safely in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmr_engineerman.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mtrain_the_engineer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     save_the_wisdom()\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mtrain_the_engineer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     38\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Step [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:111\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    105\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    113\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 7.75 GiB of which 1.56 MiB is free. Process 6608 has 5.63 GiB memory in use. Including non-PyTorch memory, this process has 2.10 GiB memory in use. Of the allocated memory 1.91 GiB is allocated by PyTorch, and 5.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from loss import DetectionLoss\n",
    "from dataloader import MrDataHead\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = MrEngineerMan().to(device)\n",
    "#model = SpecialEngineerMan().to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = DetectionLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load data\n",
    "train_loader = MrDataHead(batch_size=8)\n",
    "\n",
    "# Training loop\n",
    "def train_the_engineer():\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():,.4f}')\n",
    "                \n",
    "        print(f\"Ending Epoch {epoch+1} with Loss {loss.item():,.4f}\")\n",
    "\n",
    "# Save the model checkpoint\n",
    "def save_the_wisdom():\n",
    "    torch.save(model.state_dict(), 'mr_engineerman.ckpt')\n",
    "    print(\"Wisdom stored safely in 'mr_engineerman.ckpt'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_the_engineer()\n",
    "    save_the_wisdom()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
