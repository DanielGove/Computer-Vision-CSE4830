{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Based Rotation Invariant Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "k1qc3wD6iYH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Team Members\n",
        "- Nikolas Anagostou: Team Member\n",
        "- Daniel Gove: Team Member\n",
        "- Zachary Varnum: Team Leader"
      ],
      "metadata": {
        "id": "JSQHUaqYir_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In traditional convolutional neural networks (CNNs), the filters applied during the convolution process are sensitive to the orientation of features within the input data. This characteristic can reduce the model's effectiveness in applications where orientation is variable and not indicative of class distinctions. To address this limitation, our project proposes a novel convolutional architecture that utilizes the magnitude and phase of vectors in 2D space. These vectors represent both the convolutional response and the angle of rotation of the filter that produced that response, aiming to create a rotation-invariant CNN. This approach is expected to enhance model robustness by decoupling feature recognition from specific orientations.\n"
      ],
      "metadata": {
        "id": "UEcZ1ytKjFB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proposed Method\n",
        "\n",
        "### Concept Overview\n",
        "\n",
        "In our architecture, we explore a novel approach to achieving rotation invariance in convolutional neural networks through vector representations of the convolutional filters and responses. By utilizing both Cartesian and polar forms of vectors, our method leverages the inherent properties of these coordinate systems—vector addition in Cartesian and rotation-scaling in polar—to effectively handle orientations in image data.\n"
      ],
      "metadata": {
        "id": "QF08QIyFjwcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Representations\n",
        "\n",
        "#### Cartesian and Polar Coordinates\n",
        "\n",
        "Vectors can be represented in two primary forms:\n",
        "\n",
        "- **Cartesian Coordinates** (x, y): Ideal for operations such as vector addition. For example, to add two vectors $(x_1, y_1)$ and $(x_2, y_2)$, the resulting vector is $(x_1+x_2, y_1+y_2)$.\n",
        "  \n",
        "  ![Diagram of Vector Addition](URL_to_diagram_vector_addition)\n",
        "\n",
        "- **Polar Coordinates** (r, θ): More suited for operations involving rotations and scaling. A vector in polar form $(r, θ)$ can be rotated by an angle $\\phi$ and scaled by a factor $a$ resulting in $(ar, θ+\\phi)$.\n",
        "\n",
        "  ![Diagram of Vector Rotation and Scaling](URL_to_diagram_vector_rotation_scaling)\n",
        "\n",
        "#### Conversion between Cartesian and Polar\n",
        "\n",
        "Conversion between these coordinate systems is governed by the following formulas:\n",
        "- To Polar: $r = \\sqrt{x^2 + y^2}, \\quad \\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$\n",
        "- To Cartesian: $x = r \\cos(\\theta), \\quad y = r \\sin(\\theta)$"
      ],
      "metadata": {
        "id": "XIVanG4znFwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Convolution Operation\n",
        "\n",
        "#### Theoretical Formulation\n",
        "\n",
        "Let $F$ represent a convolutional filter intended for application on input data. For a given rotation angle $\\theta$, the filter is transformed by the rotation matrix $R_{\\theta}$, yielding the rotated filter $F \\cdot R_{\\theta}$. The convolutional response to an input $x$ under this transformation is defined as:\n",
        "$$\n",
        "f(x, \\theta) = (F \\cdot R_{\\theta}) \\ast x,\n",
        "$$\n",
        "where $\\ast$ signifies the convolution operation.\n",
        "\n",
        "To encapsulate the orientation dynamics within the convolutional framework, we introduce a vector convolution operation, formalized as follows:\n",
        "$$\n",
        "v(x, \\theta) = f(x, \\theta) \\cdot e^{i\\theta},\n",
        "$$\n",
        "which maps the convolutional output to a vector in polar coordinates $(a, \\theta)$. Here, $a$ denotes the magnitude of the response, and $\\theta$ indicates the orientation of the applied filter. The factor $e^{i\\theta}$, a unit complex number, serves to rotate the output vector by $\\theta$ radians, thereby aligning the response's orientation with that of the filter's application. This formulation leverages the rotational properties of complex numbers to seamlessly integrate orientation information into the network's output.\n",
        "\n",
        "#### Handling Negative Responses\n",
        "\n",
        "One inherent challenge in this vector convolution framework is that convolutional responses can be negative, whereas magnitudes in polar coordinates are inherently non-negative. To resolve this, we reinterpret a negative response $(-a, \\theta)$ as $(a, \\theta + \\pi)$, effectively treating the angle $\\theta$ as the orientation modulo $\\pi$. This convention, while allowing for representation of negative amplitudes, introduces ambiguity in angle $\\theta$, as it no longer uniquely identifies the filter orientation.\n"
      ],
      "metadata": {
        "id": "zVHVfoF3nH_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Rotation for Orientation Variance\n",
        "\n",
        "To robustly capture orientation variances, our model systematically applies $N$ discrete rotations to the convolutional filter $F$. These rotations are encapsulated within a tensor $R$, where each slice $R_n$ represents a rotation matrix for a specific angle $\\theta_n$. The process can be mathematically described as follows:\n",
        "\n",
        "$$\n",
        "F_n = F \\cdot R_n \\quad \\text{for} \\quad n = 1, 2, ..., N\n",
        "$$\n",
        "\n",
        "where $F_n$ represents the rotated filter corresponding to the $n$-th orientation. This sequence of operations ensures a comprehensive exploration of potential orientations, which is critical for achieving rotation invariance in the network's processing capabilities.\n",
        "\n",
        "This iterative application of rotations is encapsulated within:\n",
        "$$\n",
        "\\mathcal{R}(F) = \\{F \\cdot R_1, F \\cdot R_2, \\dots, F \\cdot R_N\\}\n",
        "$$\n",
        "\n",
        "Here, $\\mathcal{R}(F)$ denotes the set of all rotated versions of the filter $F$, each transformed by a rotation matrix $R_n$ corresponding to an angle $\\theta_n$. This methodical rotation facilitates not only a thorough analysis of orientation dynamics but also forms the foundation for the rotation-invariant properties of our network.\n",
        "\n",
        "By methodically iterating over these orientations, the network ensures it maintains high performance irrespective of the angular disposition of input features. This capability is particularly valuable in applications where feature orientation varies significantly, thereby influencing the accuracy of the convolutional analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "fiVY1a6andcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Depthwise Magnitude Max Pooling\n",
        "\n",
        "Following the iterative rotation convolution step, our architecture incorporates a depthwise magnitude max pooling operation. This process can be mathematically represented as follows:\n",
        "\n",
        "Let $V$ be the tensor output by the vector convolution operation across multiple orientations, where $V$ has dimensions $[H, W, N, C]$:\n",
        "- $H$ and $W$ are the height and width of the feature map,\n",
        "- $N$ is the number of different orientations,\n",
        "- $C$ is the number of channels (filters).\n",
        "\n",
        "The depthwise magnitude max pooling operates on the magnitude of the responses from different orientations for each spatial location $(i, j)$ and each channel $c$. Mathematically, it is defined as:\n",
        "$$\n",
        "M_{ijc} = \\max_{n=1}^N |V_{ijnc}|,\n",
        "$$\n",
        "where $|V_{ijnc}|$ denotes the magnitude of the vector response at position $(i, j)$, for orientation $n$, and channel $c$. The operation selects the maximum magnitude across all orientations $N$, preserving the orientation $\\theta_n$ corresponding to this maximal response:\n",
        "$$\n",
        "\\Theta_{ijc} = \\theta_n \\quad \\text{such that} \\quad n = \\operatorname{argmax}_{n=1}^N |V_{ijnc}|.\n",
        "$$\n",
        "\n",
        "This pooling method effectively identifies and preserves the most prominent orientation for each filter at each spatial location, optimizing the filter's detection capabilities in the processed input. The output tensor $M$ with elements $M_{ijc}$ represents the pooled feature map, and $\\Theta$ with elements $\\Theta_{ijc}$ captures the optimal orientations."
      ],
      "metadata": {
        "id": "N8EBAig_ofYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Feature Map Integration and Transformation\n",
        "\n",
        "#### Applying Convolution Weights in Polar Form\n",
        "\n",
        "Given that the vector feature maps are stored in polar form $(r, \\theta)$, using polar coordinates for applying convolutional weights offers clear advantages for rotation-sensitive tasks:\n",
        "- **Simplified Rotation Handling**: Rotations are managed by simply adjusting the angle component $\\theta$, which is inherently simpler and more efficient than the Cartesian coordinate transformations.\n",
        "- **Direct Manipulation of Magnitude and Phase**: Polar coordinates allow direct scaling of magnitudes and straightforward rotational shifts by modifying $\\theta$, aligning features based on their orientation.\n",
        "\n",
        "The convolution weights are also in polar form $(a_i, \\theta_i)$, and multiplication of two polar coordinates is performed as:\n",
        "$$\n",
        "r' = r \\cdot a_i, \\quad \\theta' = \\theta + \\theta_i\n",
        "$$\n",
        "This results in a new vector $(r', \\theta')$, effectively scaling the magnitude and rotating the phase of each input vector. This operation is optimal for tasks where the alignment and scale relative to different orientations are crucial.\n",
        "\n",
        "#### Summation and Feature Interaction in Cartesian Coordinates\n",
        "\n",
        "After applying the weights, the vectors, now transformed to $(r', \\theta')$, are converted to Cartesian coordinates to facilitate summation:\n",
        "$$\n",
        "x = r' \\cos(\\theta'), \\quad y = r' \\sin(\\theta')\n",
        "$$\n",
        "The sum of these Cartesian components across all applied filters is computed for each feature map location:\n",
        "$$\n",
        "X = \\sum_{i=1}^N x_i, \\quad Y = \\sum_{i=1}^N y_i\n",
        "$$\n",
        "where $N$ is the number of filters or transformations applied. This summation allows for the aggregation of features from various orientations, enhancing coherent (aligned) features and reducing incoherent (misaligned) ones.\n",
        "\n",
        "After summing, the average is typically taken to normalize the results, especially in deep convolutional layers where managing feature scale is critical:\n",
        "$$\n",
        "X_{avg} = \\frac{X}{N}, \\quad Y_{avg} = \\frac{Y}{N}\n",
        "$$\n",
        "This averaging process helps in stabilizing the learning by reducing the variance of the output values, making the model less sensitive to the specific number of filters used.\n",
        "\n",
        "#### Reconversion to Polar and Introduction of Non-Linearity\n",
        "\n",
        "The summed and averaged Cartesian components $(X_{avg}, Y_{avg})$ are then converted back into polar coordinates to prepare for non-linear activation:\n",
        "$$\n",
        "R = \\sqrt{X_{avg}^2 + Y_{avg}^2}, \\quad \\Phi = \\operatorname{atan2}(Y_{avg}, X_{avg})\n",
        "$$\n",
        "A polar ReLU function is applied to introduce non-linearity, crucial for capturing non-linear relationships within the data:\n",
        "$$\n",
        "R' = \\begin{cases}\n",
        "R & \\text{if } R \\geq 1 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "This operation enhances model complexity and discriminative power by ensuring that only significant magnitudes contribute to the network’s further layers."
      ],
      "metadata": {
        "id": "1AhyHlCGsmWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further Processing and Integration into Traditional Architectures\n",
        "\n",
        "#### Transition to Standard Convolutional Layers\n",
        "\n",
        "Following the specialized operations of applying convolutional weights in polar form and converting between coordinate systems, the feature maps are processed using standard convolutional layers. These layers are defined mathematically by:\n",
        "$$\n",
        "F_{out} = \\sigma\\left(W * F_{in} + b\\right)\n",
        "$$\n",
        "where $*$ denotes the convolution operation, $W$ represents the convolutional kernels, $b$ is the bias, $F_{in}$ is the input feature map from the previous polar operations, $\\sigma$ is the activation function (e.g., ReLU), and $F_{out}$ is the output feature map.\n",
        "\n",
        "#### Integration of Pooling Layers\n",
        "\n",
        "To reduce spatial dimensions and enhance feature robustness against small variations and noise, pooling layers are integrated:\n",
        "$$\n",
        "P_{out} = \\text{pool}(F_{out})\n",
        "$$\n",
        "Here, $\\text{pool}$ can be a max pooling operation where the maximum value within a specified window is selected:\n",
        "$$\n",
        "P_{out}(i, j) = \\max_{a, b \\in W}(F_{out}(i+a, j+b))\n",
        "$$\n",
        "This operation reduces the size of each feature map while preserving the most prominent features, enhancing translational invariance.\n",
        "\n",
        "#### Applying Magnitude Max Pooling for Downsampling\n",
        "\n",
        "Additionally, to focus on the most relevant features in terms of magnitude, especially after handling complex transformations:\n",
        "$$\n",
        "M_{out} = \\max_{\\theta \\in \\Theta}( |F_{\\theta}| )\n",
        "$$\n",
        "where $|F_{\\theta}|$ represents the magnitude of the vector feature at each point, considering different orientations $\\Theta$. This magnitude max pooling helps in downsampling the feature maps by selecting the dominant features across different rotations, thus reinforcing the network's rotation invariance.\n",
        "\n",
        "#### Normalization and Final Layers\n",
        "\n",
        "Before proceeding to the output layers, normalization techniques such as Batch Normalization can be applied:\n",
        "$$\n",
        "F_{norm} = \\gamma \\left(\\frac{F_{out} - \\mu_{B}}{\\sqrt{\\sigma^2_{B} + \\epsilon}}\\right) + \\beta\n",
        "$$\n",
        "where $\\mu_B$ and $\\sigma^2_B$ are the mean and variance of the batch, $\\epsilon$ is a small constant to prevent division by zero, and $\\gamma$, $\\beta$ are parameters learned during training to scale and shift the normalized data.\n",
        "\n",
        "#### Output Layer and Classification\n",
        "\n",
        "Finally, the processed feature maps can be flattened and fed into a dense layer for classification, or further convolutional layers depending on the specific task:\n",
        "$$\n",
        "\\text{Output} = \\text{Softmax}(W_{final} \\cdot \\text{Flatten}(F_{norm}) + b_{final})\n",
        "$$\n",
        "This structure allows the integration of the specialized vector-based convolutional features with standard layers of a CNN, ensuring that the network is suitable for complex tasks requiring understanding of rotation-invariant features.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This integration strategy ensures that the novel polar-based convolution operations are seamlessly combined with traditional CNN architectures, allowing for effective learning and generalization across varied visual tasks. The inclusion of both standard and novel pooling methods enhances the network's ability to handle different spatial and rotational variances in input data.\n"
      ],
      "metadata": {
        "id": "EsdKsTI87gX5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1hbYbMSVCsY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}